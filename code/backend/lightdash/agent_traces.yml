# Lightdash metric definitions — agent_traces explore
#
# Dashboard panel: Agent Learning Curve
# Shows average quality score per agent over time — the visual proof that
# the self-improvement loops are working.

version: 2

models:
  - name: agent_traces
    label: "Agent Traces"
    description: >
      Execution records for every agent run.  Braintrust trace IDs are stored
      here for deep-dive evaluation.  Quality scores are attached after
      each campaign's performance is measured.

    columns:
      - name: id
        label: "Trace ID"
        description: "Unique trace identifier."

      - name: agent_name
        label: "Agent"
        description: "Name of the agent that produced this trace."
        meta:
          dimension:
            type: string

      - name: braintrust_trace_id
        label: "Braintrust Trace ID"
        description: "Cross-reference to Braintrust for LLM-level debugging."

      - name: company_id
        label: "Company ID"
        description: "Company this trace belongs to."
        meta:
          dimension:
            type: string

      - name: quality_score
        label: "Quality Score"
        description: >
          Composite quality score (0–1) computed by the Feedback Loop Agent
          from campaign engagement and safety metrics.  Rising scores over time
          indicate successful self-improvement.
        meta:
          metrics:
            avg_quality_score:
              type: average
              label: "Avg Quality Score"
              description: "Average quality across all agent runs in the period."
              round: 3
            min_quality_score:
              type: min
              label: "Min Quality Score"
              round: 3
            max_quality_score:
              type: max
              label: "Peak Quality Score"
              round: 3

      - name: latency_ms
        label: "Latency (ms)"
        description: "Wall-clock time for the full agent run."
        meta:
          metrics:
            avg_latency_ms:
              type: average
              label: "Avg Latency (ms)"
              round: 0
            p95_latency_ms:
              type: percentile
              percentile: 95
              label: "p95 Latency (ms)"
              round: 0

      - name: tokens_used
        label: "Tokens Used"
        description: "Total tokens consumed (input + output)."
        meta:
          metrics:
            avg_tokens_used:
              type: average
              label: "Avg Tokens"
              round: 0
            total_tokens_used:
              type: sum
              label: "Total Tokens"

      - name: created_at
        label: "Run At"
        description: "Timestamp when the agent run started."
        meta:
          dimension:
            type: timestamp
            time_intervals: ["DAY", "WEEK", "MONTH"]
          metrics:
            count:
              type: count
              label: "Total Runs"
