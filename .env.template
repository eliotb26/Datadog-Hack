# ============================================================
# SIGNAL — .env.template
# Self-Improving Content Intelligence Platform
# ============================================================
# Copy this file to .env and fill in your values.
# NEVER commit .env to version control.
# ============================================================


# ------------------------------------------------------------
# GOOGLE DEEPMIND — Core Intelligence Layer
# Gemini API + Agent Development Kit (ADK)
# Get API key: https://aistudio.google.com/apikey
# ADK docs: https://google.github.io/adk-docs/get-started/
# ------------------------------------------------------------
GEMINI_API_KEY=your_gemini_api_key_here
# GOOGLE_API_KEY is the alternative name (takes precedence if both set)
# GOOGLE_API_KEY=your_google_api_key_here

# Set to FALSE to use Gemini API (AI Studio); TRUE to use Vertex AI
GOOGLE_GENAI_USE_VERTEXAI=FALSE

# Required only if using Vertex AI instead of AI Studio
# GOOGLE_CLOUD_PROJECT=your_gcp_project_id
# GOOGLE_CLOUD_LOCATION=us-central1


# ------------------------------------------------------------
# BRAINTRUST — Evaluation & Self-Improvement Engine
# Tracing, Loop Agent, AI Proxy for all LLM calls
# Get API key: https://www.braintrust.dev/app/settings?subroute=api-keys
# Docs: https://www.braintrust.dev/docs/guides/api
# ------------------------------------------------------------
BRAINTRUST_API_KEY=your_braintrust_api_key_here

# Optional: Braintrust project name for organizing traces
BRAINTRUST_PROJECT=signal


# ------------------------------------------------------------
# AIRIA — Enterprise Orchestration & AI Gateway
# Agent Studio workflows, A/B testing (Battleground), constraints
# Sign up: https://airia.com
# Docs: https://explore.airia.com/building-and-deploying-agents/interface-options/api-deployment
# ------------------------------------------------------------
AIRIA_API_KEY=your_airia_api_key_here
AIRIA_PIPELINE_URL=https://api.airia.com/v1/pipelines/your_pipeline_id


# ------------------------------------------------------------
# MODULATE AI — Content Safety Layer (ToxMod)
# Pre-publication toxicity screening + Appeals API
# Sign up / docs: https://cloud-console.modulate.ai/
# SDK docs: https://sdk-docs.modulate.ai/
# ------------------------------------------------------------
MODULATE_API_KEY=your_modulate_toxmod_api_key_here

# Toxicity score threshold — campaigns scoring above this are blocked (0.0–1.0)
SAFETY_THRESHOLD=0.3


# ------------------------------------------------------------
# FLORA AI — Creative Asset Generation
# 40+ image generation models (FLUX, Stable Diffusion, etc.)
# Sign up: https://www.florafauna.ai/sign-up
# Docs: https://docs.florafauna.ai/more/api
# NOTE: Flora API is currently in early access — request access via docs
# ------------------------------------------------------------
FLORA_API_KEY=your_flora_ai_api_key_here
FLORA_DEFAULT_MODEL=flux-pro
FLORA_DEFAULT_ASPECT_RATIO=16:9


# ------------------------------------------------------------
# DATADOG — Full-Stack Observability
# APM, Logs, Custom Metrics, Monitors & Dashboards
# Get API key: https://app.datadoghq.com/organization-settings/api-keys
# Get APP key: https://app.datadoghq.com/organization-settings/application-keys
# ddtrace docs: https://ddtrace.readthedocs.io/
# ------------------------------------------------------------
DD_API_KEY=your_datadog_api_key_here

# Application key required for Datadog Management API (read operations)
DD_APP_KEY=your_datadog_app_key_here

# Datadog site — use datadoghq.com (US1) or your regional site
# Options: datadoghq.com | us3.datadoghq.com | eu.datadoghq.com | ap1.datadoghq.com
DD_SITE=datadoghq.com

# Service & environment tags (appear in APM and dashboards)
DD_SERVICE=signal-backend
DD_ENV=hackathon
DD_VERSION=0.1.0

# Datadog Agent host — use 'datadog-agent' in Docker Compose, 'localhost' for local dev
DD_AGENT_HOST=datadog-agent

# APM trace intake port
DD_TRACE_AGENT_PORT=8126

# Enable APM tracing
DD_TRACE_ENABLED=true

# Enable runtime metrics (CPU, memory per service)
DD_RUNTIME_METRICS_ENABLED=true

# Enable log injection (correlates logs with traces)
DD_LOGS_INJECTION=true


# ------------------------------------------------------------
# LIGHTDASH — BI Dashboard (Self-Hosted)
# Campaign analytics, agent learning curves, cross-company patterns
# Docs: https://docs.lightdash.com/self-host/self-host-lightdash-docker-compose
# Env vars: https://docs.lightdash.com/self-host/customize-deployment/environment-variables
# WARNING: Never change LIGHTDASH_SECRET after first run — data loss will occur
# ------------------------------------------------------------
LIGHTDASH_SECRET=generate_a_strong_random_secret_here

# Lightdash PostgreSQL database (used by self-hosted Lightdash container)
PGHOST=db
PGPORT=5432
PGUSER=postgres
PGPASSWORD=your_postgres_password_here
PGDATABASE=postgres


# ------------------------------------------------------------
# POLYMARKET — Prediction Market Data Source
# No API key required — public Gamma API
# Base URL: https://gamma-api.polymarket.com
# ------------------------------------------------------------
POLYMARKET_BASE_URL=https://gamma-api.polymarket.com

# Volume velocity threshold for filtering high-momentum signals
POLYMARKET_VOLUME_THRESHOLD=10000

# Polling interval in seconds (how often the Trend Intel Agent polls)
POLYMARKET_POLL_INTERVAL_SECONDS=300


# ------------------------------------------------------------
# APPLICATION CONFIG
# FastAPI backend + React frontend settings
# ------------------------------------------------------------
# Backend
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
FRONTEND_URL=http://localhost:3000

# SQLite database path (relative to /app in Docker, or local path for dev)
DATABASE_URL=sqlite+aiosqlite:///./data/signal.db

# Environment: development | hackathon | production
ENVIRONMENT=hackathon

# Logging level: DEBUG | INFO | WARNING | ERROR
LOG_LEVEL=INFO

# Autonomous posting trust level (1=human review only, 2=semi-auto, 3=full auto)
AUTONOMOUS_TRUST_LEVEL=1

# Number of campaign concepts to generate per pipeline cycle
CAMPAIGNS_PER_CYCLE=3
